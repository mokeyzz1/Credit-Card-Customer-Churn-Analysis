{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be081bb2",
   "metadata": {},
   "source": [
    "# Credit Card Customer Churn Analysis\n",
    "\n",
    "## Business Problem\n",
    "Credit card companies lose substantial revenue when customers churn. Acquiring a new customer is far more expensive than retaining an existing one, and high churn rates directly reduce profitability. Predicting which customers are likely to leave enables the company to target retention offers more effectively, lowering churn and maximizing return on investment.\n",
    "\n",
    "## Research Questions\n",
    "- Which customer characteristics are most predictive of churn?\n",
    "- Can we build a model to identify at-risk customers before they leave?\n",
    "- What actionable insights can guide retention strategies?\n",
    "\n",
    "## Objective\n",
    "Develop a predictive model to identify customers with high churn probability and provide data-driven recommendations for retention efforts. Quantify the cost-benefit of targeting at-risk customers to demonstrate measurable business ROI from predictive analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2dd6c",
   "metadata": {},
   "source": [
    "## Stage 1: Data Discovery & Understanding\n",
    "\n",
    "Let's start by exploring what data we have available and understanding its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf92774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a62d9b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (10000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/credit_card_customers.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3d007f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education_level</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>dependents</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_limit</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>relationship_duration_years</th>\n",
       "      <th>avg_monthly_spend</th>\n",
       "      <th>monthly_balance</th>\n",
       "      <th>credit_utilization</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>cash_advance_count</th>\n",
       "      <th>card_category</th>\n",
       "      <th>late_payment_count</th>\n",
       "      <th>missed_payment_flag</th>\n",
       "      <th>last_payment_amount</th>\n",
       "      <th>online_services_used</th>\n",
       "      <th>reward_points</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST00001</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Married</td>\n",
       "      <td>2</td>\n",
       "      <td>45914.0</td>\n",
       "      <td>13900.0</td>\n",
       "      <td>34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>3186.0</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2946.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8938.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST00002</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Married</td>\n",
       "      <td>3</td>\n",
       "      <td>43534.0</td>\n",
       "      <td>8900.0</td>\n",
       "      <td>25</td>\n",
       "      <td>1.4</td>\n",
       "      <td>789.0</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>Standard</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8241.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST00003</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Married</td>\n",
       "      <td>2</td>\n",
       "      <td>49836.0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.1</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>316.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST00004</td>\n",
       "      <td>Female</td>\n",
       "      <td>30</td>\n",
       "      <td>High School</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>28599.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>134</td>\n",
       "      <td>10.7</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>0.2594</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>Standard</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9699.0</td>\n",
       "      <td>337.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST00005</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>High School</td>\n",
       "      <td>Single</td>\n",
       "      <td>0</td>\n",
       "      <td>36715.0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.9</td>\n",
       "      <td>635.0</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1282.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9047.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  gender  age education_level marital_status  dependents  \\\n",
       "0   CUST00001    Male   27       Associate        Married           2   \n",
       "1   CUST00002  Female   31       Associate        Married           3   \n",
       "2   CUST00003  Female   49       Associate        Married           2   \n",
       "3   CUST00004  Female   30     High School         Single           1   \n",
       "4   CUST00005    Male   58     High School         Single           0   \n",
       "\n",
       "    income  credit_limit  tenure_months  relationship_duration_years  \\\n",
       "0  45914.0       13900.0             34                          2.0   \n",
       "1  43534.0        8900.0             25                          1.4   \n",
       "2  49836.0        7800.0             13                          1.1   \n",
       "3  28599.0        6700.0            134                         10.7   \n",
       "4  36715.0        7800.0             34                          1.9   \n",
       "\n",
       "   avg_monthly_spend  monthly_balance  credit_utilization  transaction_count  \\\n",
       "0              630.0           3186.0              0.2292                 41   \n",
       "1              789.0           2516.0              0.2827                 33   \n",
       "2              942.0           2229.0              0.2858                 34   \n",
       "3              171.0           1738.0              0.2594                 36   \n",
       "4              635.0           1838.0              0.2356                 44   \n",
       "\n",
       "   cash_advance_count card_category  late_payment_count  missed_payment_flag  \\\n",
       "0                   1      Business                   1                    0   \n",
       "1                   3      Standard                   1                    0   \n",
       "2                   1      Business                   2                    0   \n",
       "3                   1      Standard                   2                    0   \n",
       "4                   0      Standard                   1                    0   \n",
       "\n",
       "   last_payment_amount  online_services_used  reward_points  risk_score  churn  \n",
       "0               2946.0                     7         8938.0       300.0      0  \n",
       "1                930.0                     6         8241.0       311.0      0  \n",
       "2               1069.0                     5         5108.0       316.4      0  \n",
       "3                885.0                     2         9699.0       337.1      0  \n",
       "4               1282.0                     0         9047.0       300.0      0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First look at the data\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74151543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Information:\n",
      "==================================================\n",
      " 1. customer_id\n",
      " 2. gender\n",
      " 3. age\n",
      " 4. education_level\n",
      " 5. marital_status\n",
      " 6. dependents\n",
      " 7. income\n",
      " 8. credit_limit\n",
      " 9. tenure_months\n",
      "10. relationship_duration_years\n",
      "11. avg_monthly_spend\n",
      "12. monthly_balance\n",
      "13. credit_utilization\n",
      "14. transaction_count\n",
      "15. cash_advance_count\n",
      "16. card_category\n",
      "17. late_payment_count\n",
      "18. missed_payment_flag\n",
      "19. last_payment_amount\n",
      "20. online_services_used\n",
      "21. reward_points\n",
      "22. risk_score\n",
      "23. churn\n"
     ]
    }
   ],
   "source": [
    "# Column information\n",
    "print(\"Column Information:\")\n",
    "print(\"=\"*50)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86f9284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   customer_id                  10000 non-null  object \n",
      " 1   gender                       10000 non-null  object \n",
      " 2   age                          10000 non-null  int64  \n",
      " 3   education_level              10000 non-null  object \n",
      " 4   marital_status               10000 non-null  object \n",
      " 5   dependents                   10000 non-null  int64  \n",
      " 6   income                       9950 non-null   float64\n",
      " 7   credit_limit                 10000 non-null  float64\n",
      " 8   tenure_months                10000 non-null  int64  \n",
      " 9   relationship_duration_years  10000 non-null  float64\n",
      " 10  avg_monthly_spend            10000 non-null  float64\n",
      " 11  monthly_balance              10000 non-null  float64\n",
      " 12  credit_utilization           10000 non-null  float64\n",
      " 13  transaction_count            10000 non-null  int64  \n",
      " 14  cash_advance_count           10000 non-null  int64  \n",
      " 15  card_category                10000 non-null  object \n",
      " 16  late_payment_count           10000 non-null  int64  \n",
      " 17  missed_payment_flag          10000 non-null  int64  \n",
      " 18  last_payment_amount          9950 non-null   float64\n",
      " 19  online_services_used         10000 non-null  int64  \n",
      " 20  reward_points                9950 non-null   float64\n",
      " 21  risk_score                   10000 non-null  float64\n",
      " 22  churn                        10000 non-null  int64  \n",
      "dtypes: float64(9), int64(9), object(5)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data types and basic info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b2015919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Issue Check:\n",
      "==================================================\n",
      "Tenure vs Relationship Duration Analysis:\n",
      "tenure_months mean: 36.1\n",
      "relationship_duration_years mean: 2.2\n",
      "Expected years from tenure_months: 3.0\n",
      "Correlation between fields: 0.938\n",
      "ISSUE IDENTIFIED: tenure_months and relationship_duration_years are inconsistent\n",
      "This needs to be fixed in feature engineering stage\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Check: Tenure vs Relationship Duration Consistency\n",
    "print(\"Data Quality Issue Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if tenure_months and relationship_duration_years are consistent\n",
    "tenure_months = df['tenure_months']\n",
    "relationship_years = df['relationship_duration_years']\n",
    "expected_years = tenure_months / 12\n",
    "\n",
    "correlation = tenure_months.corr(relationship_years)\n",
    "print(\"Tenure vs Relationship Duration Analysis:\")\n",
    "print(\"tenure_months mean:\", tenure_months.mean().round(1))\n",
    "print(\"relationship_duration_years mean:\", relationship_years.mean().round(1))\n",
    "print(\"Expected years from tenure_months:\", expected_years.mean().round(1))\n",
    "print(\"Correlation between fields:\", correlation.round(3))\n",
    "\n",
    "if correlation < 0.99:\n",
    "    print(\"ISSUE IDENTIFIED: tenure_months and relationship_duration_years are inconsistent\")\n",
    "    print(\"This needs to be fixed in feature engineering stage\")\n",
    "else:\n",
    "    print(\"Fields are consistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafbdd3",
   "metadata": {},
   "outputs": [],
   "source": "# Check for our target variable - churn\nprint(\"Target Variable Analysis:\")\nprint(\"=\"*30)\nprint(\"Churn distribution:\")\nprint(df['churn'].value_counts())\nprint(\"Churn rate:\", str(round(df['churn'].mean()*100, 1)) + \"%\")\n\n# Visualize churn distribution\nplt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\ndf['churn'].value_counts().plot(kind='bar', color=['lightblue', 'lightcoral'])\nplt.title('Churn Distribution (Count)')\nplt.xlabel('Churn (0=Retained, 1=Churned)')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\n\nplt.subplot(1, 2, 2)\ndf['churn'].value_counts(normalize=True).plot(kind='bar', color=['lightblue', 'lightcoral'])\nplt.title('Churn Distribution (%)')\nplt.xlabel('Churn (0=Retained, 1=Churned)')\nplt.ylabel('Percentage')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.savefig('../outputs/01_churn_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Key findings\nprint(\"Target variable identified: 'churn' (binary classification)\")\nprint(\"Churn rate of\", str(round(df['churn'].mean()*100, 1)) + \"% is realistic for credit cards\")\nprint(\"Dataset has sufficient samples for reliable modeling\")"
  },
  {
   "cell_type": "markdown",
   "id": "c0462153",
   "metadata": {},
   "source": [
    "## Stage 2: Data Quality Assessment & Cleaning\n",
    "\n",
    "Now that we understand the dataset structure, let's assess data quality and identify issues that need to be addressed before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7997c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income                 50\n",
      "last_payment_amount    50\n",
      "reward_points          50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "missing_values = df.isnull().sum()\n",
    "missing_summary = missing_values[missing_values > 0]\n",
    "print(missing_summary) if len(missing_summary) > 0 else print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b7f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender: {'Male': 5299, 'Female': 4681, 'M': 10, 'male': 5, 'F': 5}\n",
      "education_level: {'High School': 3483, 'Associate': 2954, 'Bachelor': 2112, 'Master': 981, 'Doctorate': 455, 'Masters': 5, 'Bachelors': 5, 'PhD': 5}\n",
      "marital_status: {'Married': 5927, 'Single': 2193, 'Divorced': 1560, 'Widowed': 295, 'married': 8, 'SINGLE': 7, 'widow': 5, 'Div': 5}\n",
      "card_category: {'Business': 4686, 'Standard': 3887, 'Gold': 1253, 'Platinum': 144, ' Business': 8, ' Standard': 7, 'Business ': 6, 'Standard ': 5, ' Gold': 3, 'Platinum ': 1}\n"
     ]
    }
   ],
   "source": [
    "# Categorical data inspection\n",
    "categorical_cols = ['gender', 'education_level', 'marital_status', 'card_category']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(col + \":\", df[col].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6060263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values:\n",
      "  avg_monthly_spend: 3\n",
      "  last_payment_amount: 3\n",
      "  reward_points: 2\n",
      "Balance > Credit Limit: 2\n"
     ]
    }
   ],
   "source": [
    "# Data quality checks\n",
    "print(\"Negative values:\")\n",
    "print(\"  avg_monthly_spend:\", (df['avg_monthly_spend'] < 0).sum())\n",
    "print(\"  last_payment_amount:\", (df['last_payment_amount'] < 0).sum())\n",
    "print(\"  reward_points:\", (df['reward_points'] < 0).sum())\n",
    "\n",
    "print(\"Balance > Credit Limit:\", (df['monthly_balance'] > df['credit_limit']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6e8e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset shape: (10000, 23)\n",
      "Missing values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "# Create cleaned dataset\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Handle missing values (median imputation for numerical)\n",
    "df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
    "df_clean['last_payment_amount'].fillna(df_clean['last_payment_amount'].median(), inplace=True)\n",
    "df_clean['reward_points'].fillna(df_clean['reward_points'].median(), inplace=True)\n",
    "\n",
    "# Standardize categorical values\n",
    "df_clean['gender'] = df_clean['gender'].replace({'M': 'Male', 'male': 'Male', 'F': 'Female'})\n",
    "df_clean['education_level'] = df_clean['education_level'].replace({'Bachelors': 'Bachelor', 'Masters': 'Master', 'PhD': 'Doctorate'})\n",
    "df_clean['marital_status'] = df_clean['marital_status'].replace({'married': 'Married', 'SINGLE': 'Single', 'widow': 'Widowed', 'Div': 'Divorced'})\n",
    "df_clean['card_category'] = df_clean['card_category'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Fix negative values (set to absolute value - assuming data entry errors)\n",
    "df_clean.loc[df_clean['avg_monthly_spend'] < 0, 'avg_monthly_spend'] = abs(df_clean['avg_monthly_spend'])\n",
    "df_clean.loc[df_clean['last_payment_amount'] < 0, 'last_payment_amount'] = abs(df_clean['last_payment_amount'])\n",
    "df_clean.loc[df_clean['reward_points'] < 0, 'reward_points'] = abs(df_clean['reward_points'])\n",
    "\n",
    "print(\"Cleaned dataset shape:\", df_clean.shape)\n",
    "print(\"Missing values remaining:\", df_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec969daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical values after cleaning:\n",
      "gender: ['Male' 'Female']\n",
      "education_level: ['Associate' 'High School' 'Bachelor' 'Master' 'Doctorate']\n",
      "marital_status: ['Married' 'Single' 'Divorced' 'Widowed']\n",
      "card_category: ['Business' 'Standard' 'Gold' 'Platinum']\n",
      "Negative values after cleaning:\n",
      "  avg_monthly_spend: 0\n",
      "  last_payment_amount: 0\n",
      "  reward_points: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify cleaning results\n",
    "print(\"Categorical values after cleaning:\")\n",
    "for col in categorical_cols:\n",
    "    print(col + \":\", df_clean[col].unique())\n",
    "\n",
    "print(\"Negative values after cleaning:\")\n",
    "print(\"  avg_monthly_spend:\", (df_clean['avg_monthly_spend'] < 0).sum())\n",
    "print(\"  last_payment_amount:\", (df_clean['last_payment_amount'] < 0).sum())\n",
    "print(\"  reward_points:\", (df_clean['reward_points'] < 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74199eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to '../data/credit_card_customers_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "df_clean.to_csv('../data/credit_card_customers_cleaned.csv', index=False)\n",
    "print(\"Cleaned dataset saved to '../data/credit_card_customers_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94a86e",
   "metadata": {},
   "source": [
    "## Stage 3: Exploratory Data Analysis\n",
    "\n",
    "Now let's explore customer behavior patterns and identify what drives churn through data visualization and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efdc841",
   "metadata": {},
   "outputs": [],
   "source": "# Demographic analysis by churn\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Age distribution\naxes[0,0].hist([df_clean[df_clean['churn']==0]['age'], df_clean[df_clean['churn']==1]['age']], \n               bins=20, alpha=0.7, label=['Retained', 'Churned'], color=['lightblue', 'lightcoral'])\naxes[0,0].set_title('Age Distribution by Churn')\naxes[0,0].set_xlabel('Age')\naxes[0,0].set_ylabel('Count')\naxes[0,0].legend()\n\n# Gender by churn\ngender_churn = pd.crosstab(df_clean['gender'], df_clean['churn'], normalize='index')\ngender_churn.plot(kind='bar', ax=axes[0,1], color=['lightblue', 'lightcoral'])\naxes[0,1].set_title('Churn Rate by Gender')\naxes[0,1].set_ylabel('Churn Rate')\naxes[0,1].tick_params(axis='x', rotation=0)\n\n# Education by churn\neducation_churn = pd.crosstab(df_clean['education_level'], df_clean['churn'], normalize='index')\neducation_churn.plot(kind='bar', ax=axes[1,0], color=['lightblue', 'lightcoral'])\naxes[1,0].set_title('Churn Rate by Education')\naxes[1,0].set_ylabel('Churn Rate')\naxes[1,0].tick_params(axis='x', rotation=45)\n\n# Income distribution\naxes[1,1].hist([df_clean[df_clean['churn']==0]['income'], df_clean[df_clean['churn']==1]['income']], \n               bins=20, alpha=0.7, label=['Retained', 'Churned'], color=['lightblue', 'lightcoral'])\naxes[1,1].set_title('Income Distribution by Churn')\naxes[1,1].set_xlabel('Income')\naxes[1,1].set_ylabel('Count')\naxes[1,1].legend()\n\nplt.tight_layout()\nplt.savefig('../outputs/02_demographic_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898f8a8",
   "metadata": {},
   "outputs": [],
   "source": "# Financial behavior analysis\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Monthly spending\naxes[0,0].boxplot([df_clean[df_clean['churn']==0]['avg_monthly_spend'], \n                   df_clean[df_clean['churn']==1]['avg_monthly_spend']], \n                  labels=['Retained', 'Churned'])\naxes[0,0].set_title('Monthly Spending by Churn')\naxes[0,0].set_ylabel('Monthly Spend ($)')\n\n# Credit utilization\naxes[0,1].boxplot([df_clean[df_clean['churn']==0]['credit_utilization'], \n                   df_clean[df_clean['churn']==1]['credit_utilization']], \n                  labels=['Retained', 'Churned'])\naxes[0,1].set_title('Credit Utilization by Churn')\naxes[0,1].set_ylabel('Credit Utilization')\n\n# Transaction count\naxes[1,0].boxplot([df_clean[df_clean['churn']==0]['transaction_count'], \n                   df_clean[df_clean['churn']==1]['transaction_count']], \n                  labels=['Retained', 'Churned'])\naxes[1,0].set_title('Transaction Count by Churn')\naxes[1,0].set_ylabel('Transaction Count')\n\n# Credit limit\naxes[1,1].boxplot([df_clean[df_clean['churn']==0]['credit_limit'], \n                   df_clean[df_clean['churn']==1]['credit_limit']], \n                  labels=['Retained', 'Churned'])\naxes[1,1].set_title('Credit Limit by Churn')\naxes[1,1].set_ylabel('Credit Limit ($)')\n\nplt.tight_layout()\nplt.savefig('../outputs/03_financial_behavior_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb06fa",
   "metadata": {},
   "outputs": [],
   "source": "# Correlation analysis with churn\nnumerical_cols = ['age', 'income', 'credit_limit', 'tenure_months', 'avg_monthly_spend', \n                 'credit_utilization', 'transaction_count', 'late_payment_count', 'risk_score']\n\ncorrelations = df_clean[numerical_cols + ['churn']].corr()['churn'].sort_values(key=abs, ascending=False)\n\nplt.figure(figsize=(10, 6))\ncorrelations[1:].plot(kind='barh', color=['red' if x < 0 else 'blue' for x in correlations[1:]])\nplt.title('Feature Correlations with Churn')\nplt.xlabel('Correlation Coefficient')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig('../outputs/04_correlation_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Top correlations with churn:\")\nprint(correlations[1:6])"
  },
  {
   "cell_type": "markdown",
   "id": "003d7d67",
   "metadata": {},
   "source": [
    "## EDA Key Insights\n",
    "\n",
    "### Strongest Predictors of Churn:\n",
    "1. **Tenure** (-0.18 correlation): Newer customers more likely to churn\n",
    "2. **Income** (-0.07): Lower income customers show slightly higher churn\n",
    "3. **Credit Limit** (-0.07): Lower credit limits associated with higher churn\n",
    "4. **Risk Score** (+0.05): Higher risk scores correlate with churn\n",
    "5. **Age** (+0.04): Older customers slightly more likely to churn\n",
    "\n",
    "### Business Insights:\n",
    "- Customer retention critical in first year of relationship\n",
    "- High-value customers (higher income, credit limits) more loyal\n",
    "- Middle-aged customers represent key churn segment\n",
    "- Credit utilization and transaction patterns show clear differences\n",
    "- Monthly spending significantly lower for churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f91d2f",
   "metadata": {},
   "source": [
    "# Stage 4: Feature Engineering\n",
    "\n",
    "## Creating new features to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caa3b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing tenure/relationship duration inconsistency identified in Stage 1...\n",
      "New features created: 6\n",
      "Data consistency issue fixed\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "# Fix the tenure vs relationship duration inconsistency identified in data quality assessment\n",
    "# Issue: correlation was 0.938 instead of 1.0, indicating inconsistent data\n",
    "print(\"Fixing tenure/relationship duration inconsistency identified in Stage 1...\")\n",
    "\n",
    "# Use tenure_months as primary source (more precise) and recalculate relationship_duration_years\n",
    "df_features['relationship_duration_years'] = df_features['tenure_months'] / 12\n",
    "\n",
    "# Create new features\n",
    "df_features['utilization_category'] = pd.cut(df_features['credit_utilization'], \n",
    "                                           bins=[0, 0.3, 0.7, 1.0], \n",
    "                                           labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "df_features['clv_proxy'] = df_features['avg_monthly_spend'] * df_features['tenure_months']\n",
    "\n",
    "df_features['spend_to_limit_ratio'] = df_features['avg_monthly_spend'] / df_features['credit_limit']\n",
    "\n",
    "df_features['engagement_score'] = (df_features['transaction_count'] * \n",
    "                                 df_features['avg_monthly_spend'] / 1000)\n",
    "\n",
    "df_features['risk_adjusted_score'] = df_features['risk_score'] / df_features['credit_limit'] * 1000\n",
    "\n",
    "df_features['age_group'] = pd.cut(df_features['age'], \n",
    "                                bins=[0, 35, 50, 65, 100], \n",
    "                                labels=['Young', 'Middle', 'Senior', 'Elder'])\n",
    "\n",
    "new_features = ['utilization_category', 'clv_proxy', 'spend_to_limit_ratio', \n",
    "                'engagement_score', 'risk_adjusted_score', 'age_group']\n",
    "print(\"New features created:\", len(new_features))\n",
    "print(\"Data consistency issue fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6788b919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of tenure consistency fix:\n",
      "   tenure_months  relationship_duration_years\n",
      "0             34                     2.833333\n",
      "1             25                     2.083333\n",
      "2             13                     1.083333\n",
      "3            134                    11.166667\n",
      "4             34                     2.833333\n",
      "New correlation: 1.0\n",
      "Data consistency issue resolved\n"
     ]
    }
   ],
   "source": [
    "# Verify the fix worked\n",
    "print(\"Verification of tenure consistency fix:\")\n",
    "sample_check = df_features[['tenure_months', 'relationship_duration_years']].head()\n",
    "print(sample_check)\n",
    "print(\"New correlation:\", df_features['tenure_months'].corr(df_features['relationship_duration_years']).round(6))\n",
    "print(\"Data consistency issue resolved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06bf6feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['customer_id', 'gender', 'age', 'education_level', 'marital_status', 'dependents', 'income', 'credit_limit', 'tenure_months', 'relationship_duration_years', 'avg_monthly_spend', 'monthly_balance', 'credit_utilization', 'transaction_count', 'cash_advance_count', 'card_category', 'late_payment_count', 'missed_payment_flag', 'last_payment_amount', 'online_services_used', 'reward_points', 'risk_score', 'churn', 'utilization_category', 'clv_proxy', 'spend_to_limit_ratio', 'engagement_score', 'risk_adjusted_score', 'age_group']\n",
      "Training set: (8000, 18)\n",
      "Test set: (2000, 18)\n",
      "Churn rate in training: 0.111\n",
      "Churn rate in test: 0.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check available columns\n",
    "print(\"Available columns:\", df_features.columns.tolist())\n",
    "\n",
    "# Encode categorical variables (using correct column names)\n",
    "label_encoders = {}\n",
    "categorical_cols = ['gender', 'education_level', 'marital_status', 'income_category', \n",
    "                   'card_category', 'utilization_category', 'age_group']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_features.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_features[col + '_encoded'] = le.fit_transform(df_features[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Select features for modeling\n",
    "feature_cols = ['age', 'tenure_months', 'transaction_count', 'late_payment_count',\n",
    "                'avg_monthly_spend', 'credit_limit', 'credit_utilization', 'risk_score',\n",
    "                'clv_proxy', 'spend_to_limit_ratio', 'engagement_score', 'risk_adjusted_score'] + \\\n",
    "               [col + '_encoded' for col in categorical_cols if col in df_features.columns]\n",
    "\n",
    "X = df_features[feature_cols]\n",
    "y = df_features['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set:\", X_train_scaled.shape)\n",
    "print(\"Test set:\", X_test_scaled.shape)\n",
    "print(\"Churn rate in training:\", round(y_train.mean(), 3))\n",
    "print(\"Churn rate in test:\", round(y_test.mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637198a9",
   "metadata": {},
   "source": [
    "# Stage 5: Model Development\n",
    "\n",
    "## Building predictive models to identify churn risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87324dc9",
   "metadata": {},
   "source": [
    "## 5.1 Train Baseline Models\n",
    "\n",
    "We start with two simple, interpretable models:\n",
    "- **Logistic Regression** as a baseline.\n",
    "- **Gradient Boosting** as a more flexible non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b52a37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (ROC AUC):\n",
      "  Logistic Regression: 0.713\n",
      "  Gradient Boosting:   0.722\n",
      "  Difference:          0.009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lr_auc = roc_auc_score(y_test, y_prob_lr)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "y_prob_gb = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "gb_auc = roc_auc_score(y_test, y_prob_gb)\n",
    "\n",
    "# Print results\n",
    "print(\"Model Performance (ROC AUC):\")\n",
    "print(f\"  Logistic Regression: {lr_auc:.3f}\")\n",
    "print(f\"  Gradient Boosting:   {gb_auc:.3f}\")\n",
    "print(f\"  Difference:          {gb_auc - lr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea337b",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate Models\n",
    "\n",
    "We compare classification performance for both models:\n",
    "- **Classification reports** (precision, recall, F1) at the default 0.5 cutoff  \n",
    "- **Confusion matrices** (heatmaps)  \n",
    "- **ROC curve comparison** to visualize ranking quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a14f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report — Logistic Regression (0.5 threshold)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.889     1.000     0.942      1779\n",
      "           1      0.000     0.000     0.000       221\n",
      "\n",
      "    accuracy                          0.889      2000\n",
      "   macro avg      0.445     0.500     0.471      2000\n",
      "weighted avg      0.791     0.889     0.837      2000\n",
      "\n",
      "\n",
      "Classification Report — Gradient Boosting (0.5 threshold)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.892     0.990     0.938      1779\n",
      "           1      0.280     0.032     0.057       221\n",
      "\n",
      "    accuracy                          0.884      2000\n",
      "   macro avg      0.586     0.511     0.498      2000\n",
      "weighted avg      0.824     0.884     0.841      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 5.2.1 Classification Reports\n",
    "print(\"Classification Report — Logistic Regression (0.5 threshold)\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=3))\n",
    "\n",
    "print(\"\\nClassification Report — Gradient Boosting (0.5 threshold)\")\n",
    "print(classification_report(y_test, y_pred_gb, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e5251",
   "metadata": {},
   "outputs": [],
   "source": "# 5.2.2 Confusion Matrices\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nsns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues', ax=axes[0])\naxes[0].set_title('Logistic Regression — Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\n\nsns.heatmap(confusion_matrix(y_test, y_pred_gb), annot=True, fmt='d', cmap='Blues', ax=axes[1])\naxes[1].set_title('Gradient Boosting — Confusion Matrix')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\n\nplt.tight_layout()\nplt.savefig('../outputs/05_confusion_matrices.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab033569",
   "metadata": {},
   "outputs": [],
   "source": "# 5.2.3 ROC Curve Comparison\nfpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\nfpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_gb)\n\nplt.figure(figsize=(6, 5))\nplt.plot(fpr_lr, tpr_lr, label=f'LogReg (AUC = {lr_auc:.3f})')\nplt.plot(fpr_gb, tpr_gb, label=f'GradBoost (AUC = {gb_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve Comparison')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('../outputs/06_roc_curves.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "ec30e04d",
   "metadata": {},
   "source": [
    "## 5.3 Derive Business Values from Data\n",
    "\n",
    "We estimate the annual value of a customer using our dataset:\n",
    "- **AVG_VALUE** = median monthly spend × 12  \n",
    "We also set an assumption for the cost to run a retention campaign per targeted customer:\n",
    "- **RETENTION_COST** (industry average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1edb6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived annual value per customer: $6,738\n",
      "Retention cost per targeted customer: $50.0\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Derive Business Values from Data\n",
    "\n",
    "# Median monthly spend → annual value\n",
    "AVG_VALUE = df_features['avg_monthly_spend'].median() * 12\n",
    "print(f\"Derived annual value per customer: ${AVG_VALUE:,.0f}\")\n",
    "\n",
    "# Retention cost assumption (industry estimate)\n",
    "RETENTION_COST = 50.0\n",
    "print(f\"Retention cost per targeted customer: ${RETENTION_COST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c9638",
   "metadata": {},
   "source": [
    "## 5.4 Threshold Tuning for Profit\n",
    "\n",
    "We optimize the decision cutoff for **business impact**, not just accuracy.  \n",
    "For each possible threshold, we calculate:\n",
    "\n",
    "Profit = (True Positives × Annual Value per Saved Customer)  \n",
    "    − (True Positives + False Positives) × Retention Cost per Customer\n",
    "\n",
    "Where:\n",
    "- **True Positives (TP)** = actual churners correctly flagged\n",
    "- **False Positives (FP)** = non-churners incorrectly targeted\n",
    "- **Annual Value per Saved Customer** = from section 5.3\n",
    "- **Retention Cost per Customer** = from section 5.3\n",
    "\n",
    "### 5.4.1 Find profit-maximizing threshold\n",
    "- Compute precision–recall over all thresholds\n",
    "- Convert to counts (TP, FP) and calculate profit at each threshold\n",
    "- Select the threshold that maximizes profit and evaluate metrics\n",
    "\n",
    "### 5.4.2 Profit vs. Threshold Plot\n",
    "- Visualize how expected profit changes across thresholds\n",
    "- Highlight the chosen threshold\n",
    "\n",
    "### 5.4.3 Confusion Matrix @ Best Threshold\n",
    "- Show classification outcomes (TP, FP, TN, FN) at the selected operating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ae906b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for maximum profit: 0.016\n",
      "Expected profit at best threshold: $1,394,710\n",
      "\n",
      "Metrics @ best threshold\n",
      "Confusion Matrix:\n",
      " [[ 246 1533]\n",
      " [   1  220]]\n",
      "Precision: 0.125 | Recall: 0.995\n",
      "\n",
      "Classification Report @ best threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.996     0.138     0.243      1779\n",
      "           1      0.125     0.995     0.223       221\n",
      "\n",
      "    accuracy                          0.233      2000\n",
      "   macro avg      0.561     0.567     0.233      2000\n",
      "weighted avg      0.900     0.233     0.241      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob_gb)\n",
    "\n",
    "# Positive class count (number of churners)\n",
    "P = y_test.sum()\n",
    "\n",
    "# For each threshold, get predicted positives, true positives, false positives\n",
    "pred_pos = np.array([(y_prob_gb >= t).sum() for t in thresholds])\n",
    "TP = recalls[:-1] * P  # skip last recall element to align with thresholds\n",
    "FP = pred_pos - TP\n",
    "\n",
    "# Profit calculation\n",
    "profits = TP * AVG_VALUE - (TP + FP) * RETENTION_COST\n",
    "\n",
    "# Find best threshold\n",
    "best_idx = np.argmax(profits)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold for maximum profit: {best_threshold:.3f}\")\n",
    "print(f\"Expected profit at best threshold: ${profits[best_idx]:,.0f}\")\n",
    "\n",
    "# Predictions at best threshold\n",
    "y_pred_gb_best = (y_prob_gb >= best_threshold).astype(int)\n",
    "\n",
    "# Metrics\n",
    "cm_best = confusion_matrix(y_test, y_pred_gb_best)\n",
    "precision_best = cm_best[1,1] / (cm_best[1,1] + cm_best[0,1]) if (cm_best[1,1] + cm_best[0,1]) > 0 else 0\n",
    "recall_best = cm_best[1,1] / (cm_best[1,1] + cm_best[1,0]) if (cm_best[1,1] + cm_best[1,0]) > 0 else 0\n",
    "\n",
    "print(\"\\nMetrics @ best threshold\")\n",
    "print(\"Confusion Matrix:\\n\", cm_best)\n",
    "print(f\"Precision: {precision_best:.3f} | Recall: {recall_best:.3f}\")\n",
    "print(\"\\nClassification Report @ best threshold:\")\n",
    "print(classification_report(y_test, y_pred_gb_best, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2497e2c",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.plot(thresholds, profits, label='Profit', linewidth=2)\nplt.axvline(best_threshold, linestyle='--', color='red', label=f'Best Threshold ({best_threshold:.3f})')\nplt.title(\"Profit vs. Decision Threshold\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit ($)\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('../outputs/07_profit_vs_threshold.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f2b03",
   "metadata": {},
   "outputs": [],
   "source": "import seaborn as sns\n\nplt.figure(figsize=(5,4))\nsns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues')\nplt.title(f'GB Confusion Matrix @ Threshold={best_threshold:.3f}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.tight_layout()\nplt.savefig('../outputs/08_optimal_threshold_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "16d19e9d",
   "metadata": {},
   "source": [
    "## 5.5 ROI Analysis at Best Threshold\n",
    "\n",
    "We translate the model results into **real business impact** using the optimal decision threshold from Section 5.4.\n",
    "\n",
    "Definitions:\n",
    "- **True Positives (TP)** → correctly predicted churners (saved customers)\n",
    "- **False Positives (FP)** → non-churners incorrectly targeted\n",
    "- **Annual Value per Saved Customer** → from Section 5.3\n",
    "- **Retention Cost per Customer** → from Section 5.3\n",
    "- **Acquisition Cost** → cost to replace a lost customer\n",
    "\n",
    "We calculate:\n",
    "1. **Customers Saved** = TP × (Retention Success Rate)\n",
    "2. **Revenue Saved** = Customers Saved × Annual Value\n",
    "3. **Acquisition Cost Avoided** = Customers Saved × Acquisition Cost\n",
    "4. **Campaign Cost** = (TP + FP) × Retention Cost\n",
    "5. **Net ROI** = (Revenue Saved + Acquisition Cost Avoided) − Campaign Cost\n",
    "6. **ROI Ratio** = (Total Benefits) ÷ Campaign Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ROI ANALYSIS @ BEST THRESHOLD\n",
      "============================================================\n",
      "True Positives (TP): 220\n",
      "False Positives (FP): 1533\n",
      "Customers saved (after success rate): 132\n",
      "Revenue saved: $889,416\n",
      "Acquisition cost avoided: $33,000\n",
      "Total benefits: $922,416\n",
      "Campaign cost: $87,650\n",
      "Net ROI: $834,766\n",
      "ROI ratio: 10.52x\n"
     ]
    }
   ],
   "source": [
    "# Business constants \n",
    "ACQ_COST = 250.0  # Acquisition cost to replace a lost customer\n",
    "SUCCESS_RATE = 0.6  # Retention campaign success rate\n",
    "\n",
    "# Confusion matrix values from best threshold\n",
    "TP_best = cm_best[1,1]\n",
    "FP_best = cm_best[0,1]\n",
    "\n",
    "# Customers saved after campaign success rate\n",
    "customers_saved = TP_best * SUCCESS_RATE\n",
    "\n",
    "# Financial impact\n",
    "revenue_saved = customers_saved * AVG_VALUE\n",
    "acquisition_cost_avoided = customers_saved * ACQ_COST\n",
    "campaign_cost = (TP_best + FP_best) * RETENTION_COST\n",
    "\n",
    "total_benefits = revenue_saved + acquisition_cost_avoided\n",
    "net_roi = total_benefits - campaign_cost\n",
    "roi_ratio = total_benefits / campaign_cost if campaign_cost > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*60)\n",
    "print(\"ROI ANALYSIS @ BEST THRESHOLD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Positives (TP): {TP_best}\")\n",
    "print(f\"False Positives (FP): {FP_best}\")\n",
    "print(f\"Customers saved (after success rate): {customers_saved:.0f}\")\n",
    "print(f\"Revenue saved: ${revenue_saved:,.0f}\")\n",
    "print(f\"Acquisition cost avoided: ${acquisition_cost_avoided:,.0f}\")\n",
    "print(f\"Total benefits: ${total_benefits:,.0f}\")\n",
    "print(f\"Campaign cost: ${campaign_cost:,.0f}\")\n",
    "print(f\"Net ROI: ${net_roi:,.0f}\")\n",
    "print(f\"ROI ratio: {roi_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65483242",
   "metadata": {},
   "outputs": [],
   "source": "# Simple ROI bar chart\nimport matplotlib.pyplot as plt\n\ncategories = [\"Revenue Saved\", \"Acq. Cost Avoided\", \"Campaign Cost\", \"Net ROI\"]\nvalues = [revenue_saved, acquisition_cost_avoided, -campaign_cost, net_roi]\ncolors = ['#4CAF50', '#2196F3', '#F44336', '#9C27B0']\n\nplt.figure(figsize=(8,5))\nbars = plt.bar(categories, values, color=colors)\nplt.title(\"ROI Breakdown @ Best Threshold\", fontsize=14, fontweight='bold')\nplt.ylabel(\"Amount ($)\")\n\n# Add labels\nfor bar, value in zip(bars, values):\n    plt.text(bar.get_x() + bar.get_width()/2, value + (max(values)*0.02),\n             f\"${abs(value):,.0f}\", ha='center', fontweight='bold')\n\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.savefig('../outputs/09_roi_breakdown.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "5c19422c",
   "metadata": {},
   "source": [
    "## 5.6 Sensitivity Analysis\n",
    "\n",
    "Goal: show how ROI changes if our business assumptions change.\n",
    "\n",
    "We vary:\n",
    "- **Retention Cost** per targeted customer\n",
    "- **Retention Success Rate** on true churners\n",
    "- (Optional) **Include/Exclude Acquisition Cost Avoided**\n",
    "\n",
    "We also compute **break-even points**:\n",
    "- *Break-even retention cost* — the highest we can spend per customer and still break even\n",
    "- *Break-even success rate* — the minimum success rate needed to break even at a given cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d20abff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Break-even checks (at best threshold) ===\n",
      "Current assumptions: RETENTION_COST=$50, SUCCESS_RATE=0.60, Include ACQ=True\n",
      "Break-even retention cost (given success=0.60): $526.19\n",
      "Break-even success rate (given cost=$50): 0.057\n"
     ]
    }
   ],
   "source": [
    "# 5.6.1 Break-even math at our chosen operating point\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Pull TP/FP from the confusion matrix at best threshold\n",
    "TP_best = cm_best[1,1]\n",
    "FP_best = cm_best[0,1]\n",
    "\n",
    "# Toggle whether to include acquisition cost avoided in benefits\n",
    "INCLUDE_ACQ = True\n",
    "\n",
    "value_per_save = AVG_VALUE + (ACQ_COST if INCLUDE_ACQ else 0.0)\n",
    "\n",
    "# Break-even retention cost (given success rate)\n",
    "def breakeven_retention_cost(success_rate):\n",
    "    # Net ROI = TP*s*value - (TP+FP)*RC = 0  =>  RC* = (TP*s*value)/(TP+FP)\n",
    "    denom = (TP_best + FP_best)\n",
    "    return (TP_best * success_rate * value_per_save) / denom if denom > 0 else np.nan\n",
    "\n",
    "# Break-even success rate (given retention cost)\n",
    "def breakeven_success_rate(retention_cost):\n",
    "    # 0 = TP*s*value - (TP+FP)*RC  =>  s* = ((TP+FP)*RC)/(TP*value)\n",
    "    denom = (TP_best * value_per_save)\n",
    "    return ((TP_best + FP_best) * retention_cost) / denom if denom > 0 else np.nan\n",
    "\n",
    "# Examples around your current assumptions\n",
    "RC_current = RETENTION_COST\n",
    "SR_current = 0.60  # from 5.5\n",
    "\n",
    "rc_star = breakeven_retention_cost(SR_current)\n",
    "sr_star = breakeven_success_rate(RC_current)\n",
    "\n",
    "print(\"=== Break-even checks (at best threshold) ===\")\n",
    "print(f\"Current assumptions: RETENTION_COST=${RC_current:.0f}, SUCCESS_RATE={SR_current:.2f}, \"\n",
    "      f\"Include ACQ={INCLUDE_ACQ}\")\n",
    "print(f\"Break-even retention cost (given success={SR_current:.2f}): ${rc_star:,.2f}\")\n",
    "print(f\"Break-even success rate (given cost=${RC_current:.0f}): {sr_star:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6070797",
   "metadata": {},
   "outputs": [],
   "source": "# 5.6.2 Sensitivity grid over Retention Cost and Success Rate\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nTP_best = cm_best[1,1]\nFP_best = cm_best[0,1]\nvalue_per_save = AVG_VALUE + (ACQ_COST if INCLUDE_ACQ else 0.0)\n\nretention_grid = [20, 35, 50, 75, 100, 150]\nsuccess_grid   = [0.30, 0.45, 0.60, 0.75, 0.90]\n\nrows = []\nfor rc in retention_grid:\n    for sr in success_grid:\n        customers_saved = TP_best * sr\n        benefits = customers_saved * value_per_save\n        cost = (TP_best + FP_best) * rc\n        net = benefits - cost\n        roi_ratio = (benefits / cost) if cost > 0 else np.nan\n        rows.append({\"Retention_Cost\": rc, \"Success_Rate\": sr, \"Net_ROI\": net, \"ROI_Multiple\": roi_ratio})\n\nsens_df = pd.DataFrame(rows)\n\n# Show top 10 scenarios by Net ROI\nprint(\"Top scenarios by Net ROI:\")\ndisplay(sens_df.sort_values(\"Net_ROI\", ascending=False).head(10))\n\n# Heatmap of Net ROI\nheatmap_df = sens_df.pivot(index=\"Success_Rate\", columns=\"Retention_Cost\", values=\"Net_ROI\").sort_index(ascending=True)\n\nplt.figure(figsize=(8,5))\nsns.heatmap(heatmap_df, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\nplt.title(f\"Net ROI Sensitivity (Include ACQ={INCLUDE_ACQ})\")\nplt.ylabel(\"Success Rate\")\nplt.xlabel(\"Retention Cost ($)\")\nplt.tight_layout()\nplt.savefig('../outputs/10_sensitivity_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a4ca2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quick quotes ===\n",
      "- With success rate 60% and cost $50, ROI multiple is 10.52x.\n",
      "- Break-even retention cost at 60% success is $526.19.\n",
      "- Break-even success rate at $50 cost is 5.70%.\n"
     ]
    }
   ],
   "source": [
    "# 5.6.3 Quick one-liners to quote in the write-up\n",
    "\n",
    "print(\"=== Quick quotes ===\")\n",
    "print(f\"- With success rate 60% and cost ${RETENTION_COST:.0f}, ROI multiple is \"\n",
    "      f\"{(TP_best*0.60*value_per_save)/((TP_best+FP_best)*RETENTION_COST):.2f}x.\")\n",
    "print(f\"- Break-even retention cost at 60% success is ${breakeven_retention_cost(0.60):.2f}.\")\n",
    "print(f\"- Break-even success rate at ${RETENTION_COST:.0f} cost is {breakeven_success_rate(RETENTION_COST):.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3f232",
   "metadata": {},
   "source": [
    "## Stage 5 Summary — Model Development & Business Impact\n",
    "\n",
    "**Model Comparison (ROC AUC)**  \n",
    "- Logistic Regression: 0.713  \n",
    "- Gradient Boosting: 0.722 (slightly better performance)\n",
    "\n",
    "**Best Threshold for Profit (Gradient Boosting)**  \n",
    "- Optimal cutoff: **0.016**  \n",
    "- Expected profit: **$1.39M/year**  \n",
    "- Recall = 0.995 (captures almost all churners), Precision = 0.125 (many non-churners targeted)  \n",
    "- Low precision is acceptable here due to low retention cost per customer.\n",
    "\n",
    "**ROI Analysis @ Best Threshold**  \n",
    "- Customers saved (after 60% success rate): 132  \n",
    "- Revenue saved: **$889K**  \n",
    "- Acquisition cost avoided: **$33K**  \n",
    "- Campaign cost: **$87.6K**  \n",
    "- **Net ROI: $834.8K** → ROI ratio = **10.52×**  \n",
    "\n",
    "**Sensitivity Insights**  \n",
    "- ROI remains positive even if retention cost rises to ~$500 per customer (at 60% success rate).  \n",
    "- Break-even success rate at $50 cost is only ~5.7%, meaning the campaign is highly resilient to underperformance.  \n",
    "- Higher success rates or lower costs increase profitability substantially.\n",
    "\n",
    "**Business Takeaways**  \n",
    "- The model enables **high-coverage targeting** for churn prevention at minimal financial risk.  \n",
    "- Even with low precision, cheap intervention costs make the strategy worthwhile.  \n",
    "- Key risk: unnecessary targeting of non-churners — could be improved with further precision-focused model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da606f1e",
   "metadata": {},
   "source": [
    "# Stage 6: Strategic Insights & Recommendations\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Model Performance**\n",
    "   - Gradient Boosting achieved the highest ROC AUC score (0.722), slightly outperforming Logistic Regression (0.713).  \n",
    "   - While the performance gap is modest, Gradient Boosting’s ability to better rank customers by churn probability makes it the preferred choice for operational use.\n",
    "\n",
    "2. **Threshold Tuning & Business Alignment**\n",
    "   - A profit-optimized threshold of **0.016** was identified, based on maximizing expected net ROI.\n",
    "   - At this threshold, recall is extremely high (0.995), meaning almost all true churners are captured.\n",
    "   - Precision is low (0.125), so many non-churners will also be targeted — but given the low $50 intervention cost, this is financially acceptable.\n",
    "\n",
    "3. **Financial Resilience**\n",
    "   - Current assumptions yield a **Net ROI of $834,766** and an ROI multiple of **10.52×**.\n",
    "   - Break-even analysis shows the strategy remains profitable even if retention cost per customer increases to ~$526 at a 60% success rate.\n",
    "   - Minimum success rate to break even at $50 cost is just **5.7%**, indicating strong resilience.\n",
    "\n",
    "4. **Drivers of Profitability**\n",
    "   - The main profitability driver is the **low cost of intervention** relative to the high annual value per retained customer (~$6,738).\n",
    "   - Even with a high false-positive rate, the cost-to-benefit ratio remains strongly favorable.\n",
    "\n",
    "---\n",
    "\n",
    "## Strategic Recommendations\n",
    "\n",
    "1. **Deploy the Gradient Boosting Model**\n",
    "   - Implement Gradient Boosting with a threshold of 0.016 for an initial churn prevention campaign.\n",
    "   - Ensure probability calibration is monitored to avoid drift over time.\n",
    "\n",
    "2. **Improve Campaign Success Rate**\n",
    "   - Focus marketing and retention team efforts on improving success from 60% toward 75%+.\n",
    "   - Even without improving precision, a higher success rate significantly increases ROI.\n",
    "\n",
    "3. **Segment Targeted Customers**\n",
    "   - Apply a secondary segmentation layer to predicted churners (e.g., by CLV, tenure, or product usage).\n",
    "   - Prioritize high-value or at-risk segments to maximize ROI while controlling unnecessary spend.\n",
    "\n",
    "4. **Monitor and Recalibrate**\n",
    "   - Track precision, recall, and ROI after each campaign cycle.\n",
    "   - Adjust thresholds dynamically if intervention costs change or if customer churn patterns shift.\n",
    "\n",
    "5. **Test Precision-Boosting Strategies**\n",
    "   - Explore ensemble models or post-prediction filtering to reduce false positives without significantly reducing recall.\n",
    "   - Consider dual-threshold strategies to create “high-priority” and “medium-priority” intervention tiers.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Data Enrichment:** Add additional behavioral variables (e.g., customer service interactions, payment history, product engagement) to improve model discrimination.\n",
    "- **Pilot & Measure:** Run a controlled A/B pilot comparing the targeted group against a control group to validate modeled ROI with real-world results.\n",
    "- **Sensitivity Updates:** Update cost and revenue assumptions quarterly to ensure financial models remain relevant.\n",
    "- **Operational Integration:** Work with CRM and marketing teams to automate targeting workflows and track post-intervention churn outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "> **Bottom Line:**  \n",
    "> The Gradient Boosting model with a profit-tuned threshold offers a high-recall, low-cost strategy for churn prevention that is financially robust under a wide range of conditions. While the current approach favors recall over precision, the economic upside justifies broad targeting. Continued monitoring, segmentation, and success rate improvements will further enhance ROI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}